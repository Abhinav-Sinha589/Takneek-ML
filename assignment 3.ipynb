{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6toho7uRO7v"
      },
      "source": [
        "# Question 1\n",
        "## Developing an Artificial Neural Network from Scratch.\n",
        "\n",
        "In this notebook, we will be developing a feedforward neural network.\n",
        "\n",
        "We will import the MNIST dataset from keras datsets. The MNIST dataset contains images of 28x28 pixels each having values ranging from 0-255.\n",
        "It has 60000 images in the training set and 10000 images in the test set. However, we will only use the first 10000 images for training and first 1000 images for testing because our code isn't optimized and it takes time to run. We are not looking for accuracy of our network right now, we will be doing that in the next week when we will be implementing the same using Tensorflow.\n",
        "\n",
        "\n",
        "Run the first 3 cells. Your code begins after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nI17X78rktdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrINntzulT4M",
        "outputId": "52d4ce1f-2081-45b8-854c-9afea8f59f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the class, the images are flattened to a column.\n",
        "\n",
        "Then we are normalizing them by dividing by 255."
      ],
      "metadata": {
        "id": "dr4rLzb9ZBQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0jy7CLWCEwfn"
      },
      "outputs": [],
      "source": [
        "train_X=train_X.reshape(60000,784,1)    # flattening\n",
        "test_X=test_X.reshape(10000,784,1)\n",
        "\n",
        "train_y=train_y.reshape(60000,1)\n",
        "test_y=test_y.reshape(10000,1)\n",
        "\n",
        "train_X= train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X=train_X[:10000]         #taking the first 10000 images.\n",
        "train_y=train_y[:10000]\n",
        "test_X=test_X[:1000]\n",
        "test_y=test_y[:1000]\n",
        "train_data=list(zip(train_X,train_y))\n",
        "test_data=list(zip(test_X,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Write the code for Sigmoid Function."
      ],
      "metadata": {
        "id": "wWwDzh6kZOy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Q5a8tGYku-7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sizes=[784,128,10]\n",
        "for (x,y) in zip(sizes[1:],sizes[:-1]):\n",
        "    print((x,y))\n",
        "weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "for i, weight_matrix in enumerate(weights, 1):\n",
        "    print(f\"Size of weights matrix {i}: {weight_matrix.shape}\")"
      ],
      "metadata": {
        "id": "uADAmb3shNPM",
        "outputId": "9d1245fa-fc86-47f7-b768-bbb790056d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 784)\n",
            "(10, 128)\n",
            "Size of weights matrix 1: (128, 784)\n",
            "Size of weights matrix 2: (10, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 The Network\n",
        "\n",
        "We will making a class called Network which has certain functions inside it. The cost function used is Cross-Entropy Loss. You need to code only the first 3. Rest are done for you.  There are various places within the code marked as stop_zone. Read the instructions below the code at those places to check whether your code till there is correct or not."
      ],
      "metadata": {
        "id": "cIJI5SoxbJaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RwsydmyTEt0z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self,sizes): # sizes is a list containing the network.\n",
        "                              # eg : [784,128,10] means input =784 neurons,\n",
        "                              #    1st hidden layer 128 neurons, output 10 neurons.\n",
        "        self.sizes=sizes\n",
        "        self.num_layers=len(sizes)\n",
        "        self.weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "        self.biases= [np.reshape(np.zeros(x),(x,1)) for x in sizes[1:]]#\"...can you do this by understanding the self.weights...\"\n",
        "    def show(self):\n",
        "        print(self.num_layers)\n",
        "        for bias in self.biases:\n",
        "            print(bias.shape)\n",
        "        for weight in self.weights:\n",
        "            print(weight.shape)\n",
        "# stop_zone 1. Comment out all the code below. Select all rows below. Click Ctrl + /.\n",
        "# Include the show function given below above this comment area inside the class.\n",
        "# Run this cell and then run the code with stop_zone 1 written below.\n",
        "# After this testing, don't forget tto remove the comments. Same, select all, Ctrl+/.\n",
        "\n",
        "    def forwardpropagation(self,a):\n",
        "        for b,w in zip(self.biases, self.weights):\n",
        "            a=sigmoid(np.dot(w,a)+b) # sig (w.a +b)\n",
        "            #print(a.shape)\n",
        "        return a\n",
        "\n",
        "# # stop_zone 2. Comment out all the code below. Don't comment out the __init__ method else you will get error.\n",
        "# # Remove comment from print(a.shape) line above. Run this cell. And run the code with stop_zone 2 written below.\n",
        "\n",
        "\n",
        "    def backpropagation(self,x,y):\n",
        "\n",
        "#         # nothing to do in this 3 lines.\n",
        "        y_t = np.zeros((len(y), 10))\n",
        "        y_t[np.arange(len(y)), y] = 1\n",
        "        y_t= y_t.T\n",
        "\n",
        "# #         #nabla_b=dC/db and nabla_w=dC/dw. They are lists of shapes equal to that of bias and weights.\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "#         # initially, a0 = input.\n",
        "        activation=x\n",
        "        activation_list=[x]\n",
        "\n",
        "#         # step 1 : calculation of delta in last layer\n",
        "\n",
        "#         # write the same forward propagation code here but while doing so store the a's.\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            # print(w.shape)\n",
        "            # print(activation_list[-1].shape)\n",
        "            # print(b.shape)\n",
        "            activation= sigmoid(np.dot(w,activation_list[-1])+b)\n",
        "            activation_list.append(activation)\n",
        "        delta= (activation_list[-1]-y)\n",
        "\n",
        "        # # step 2 : nabla_b and nabla_w relation with delta of last layer\n",
        "\n",
        "        nabla_b[-1]=delta\n",
        "        nabla_w[-1]= np.dot(delta,activation_list[-2].T)\n",
        "\n",
        "        #print(\"{} {}\".format(nabla_b[-1].shape,nabla_w[-1].shape) )\n",
        "# #stop_zone 3 : remove comment from the print statement just above and run the cell for stop_zone3.\n",
        "# # don't forget commenting out.\n",
        "\n",
        "        # step 3 : calculation of delta for hidden layers\n",
        "\n",
        "        for j in range(2,self.num_layers):\n",
        "            sig_der = activation_list[-j]*(1-activation_list[-j])\n",
        "            delta= (activation_list[-j]-y)#\"...how is dC/dz2 and dC/dz3 related ? Look i have calculated one term already for you (sig_der)...\"\n",
        "\n",
        "            # step 4 : nabla_b and nabla_w relation with delta of others layers\n",
        "            nabla_b[-j]= delta#\"...again, how is dC/db2 and dC/dz2 related...\"\n",
        "            nabla_w[-j]= np.dot(delta,activation_list[-j-1].T)#\"...how is dC/dw2 and dC/dz2 related...\"\n",
        "\n",
        "# #stop_zone 4 : Run the cell for stop_zone 4.\n",
        "        return (nabla_b,nabla_w)\n",
        "\n",
        "#     # the functions below are complete. If you are fine till stop_zone 4, you can run\n",
        "#     # this whole cell and train, test the data by running the last cell of the question.\n",
        "#     # You may need to wait for around 10 minutes to see the test predictions.\n",
        "    def update_mini_batch(self,mini_batch,lr):\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "        for x,y in mini_batch:\n",
        "            delta_b,delta_w= self.backpropagation(x,y)\n",
        "            nabla_b=[nb+ db for nb,db in zip (nabla_b,delta_b)]\n",
        "            nabla_w=[nw+dw for nw,dw in zip(nabla_w,delta_w)]\n",
        "\n",
        "        self.weights=[w- lr*nw/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
        "        self.biases=[b-lr*nb/len(mini_batch) for b,nb in zip(self.biases,nabla_b)]\n",
        "\n",
        "\n",
        "    def SGD(self, train_data,epochs,mini_batch_size, lr):\n",
        "        n_train= len(train_data)\n",
        "        for i in range(epochs):\n",
        "            random.shuffle(train_data)\n",
        "            mini_batches = [train_data[k:k+ mini_batch_size] for k in range(0,n_train,mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch,lr)\n",
        "\n",
        "            self.predict(train_data)\n",
        "            print(\"Epoch {0} completed.\".format(i+1))\n",
        "\n",
        "    def predict(self,test_data):\n",
        "        test_results = [(np.argmax(self.forwardpropagation(x)),y) for x,y in test_data]\n",
        "        # returns the index of that output neuron which has highest activation\n",
        "\n",
        "        num= sum(int (x==y) for x,y in test_results)\n",
        "        print (\"{0}/{1} classified correctly.\".format(num,len(test_data)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8u8cVnGamVgP",
        "outputId": "d539b1e4-1e98-4fcf-f49a-0fab9afcde5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "(128, 1)\n",
            "(64, 1)\n",
            "(10, 1)\n",
            "(128, 784)\n",
            "(64, 128)\n",
            "(10, 64)\n"
          ]
        }
      ],
      "source": [
        "# stop_zone 1\n",
        "\n",
        "# def show(self):\n",
        "#   print(self.num_layers)\n",
        "#   for bias in self.biases:\n",
        "#       print(bias.shape)\n",
        "#   for weight in self.weights:\n",
        "#       print(weight.shape)\n",
        "\n",
        "# Copy this show function from here. Paste it inside that Network Class.\n",
        "# Comment out the show function here. Run this cell.\n",
        "\n",
        "net=Network([784,128,64,10])\n",
        "net.show()\n",
        "\n",
        "# The desired output is :\n",
        "# 4\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation.\n",
        "\n",
        "# Keeping the show function over there in the Network class doesn't make any\n",
        "# difference. You may delete it if you wish. Better toss a coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D7EJBF7XsSft",
        "outputId": "4a036a58-5a31-4c89-b2fc-cd85b4366b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.99969902e-01],\n",
              "       [3.82760919e-06],\n",
              "       [7.44731833e-04],\n",
              "       [1.09294658e-01],\n",
              "       [5.31869977e-01],\n",
              "       [1.50939179e-03],\n",
              "       [2.84165626e-02],\n",
              "       [5.30038375e-01],\n",
              "       [9.55164058e-01],\n",
              "       [2.64858756e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# stop_zone 2\n",
        "# to use this, make sure your data is loaded. Run this cell.\n",
        "net=Network([784,128,64,10])\n",
        "#print(train_X[0])\n",
        "net.forwardpropagation(train_X[0])\n",
        "\n",
        "# The desired output is :\n",
        "# (784, 1)\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_zone 3\n",
        "net=Network([784,128,64,10])\n",
        "net.backpropagation(train_X[0],train_y[0])\n",
        "\n",
        "# Desired output : (10,1) (10,64)"
      ],
      "metadata": {
        "id": "FwHWyaKNhIIk",
        "outputId": "1a452097-8faf-49d5-aa70-e29b25faf1f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([[-4.99999994],\n",
              "         [-4.99937462],\n",
              "         [-4.00000919],\n",
              "         [-4.00013071],\n",
              "         [-4.53718352],\n",
              "         [-4.35161963],\n",
              "         [-4.00000001],\n",
              "         [-4.00001622],\n",
              "         [-4.99997922],\n",
              "         [-4.86452588],\n",
              "         [-4.28509708],\n",
              "         [-4.00004304],\n",
              "         [-4.25913518],\n",
              "         [-4.00065313],\n",
              "         [-4.00001324],\n",
              "         [-4.99977091],\n",
              "         [-4.00004796],\n",
              "         [-4.00694701],\n",
              "         [-4.00003693],\n",
              "         [-4.00001953],\n",
              "         [-4.57681167],\n",
              "         [-4.99901575],\n",
              "         [-4.99885559],\n",
              "         [-4.98303314],\n",
              "         [-4.00000001],\n",
              "         [-4.00104035],\n",
              "         [-4.05929701],\n",
              "         [-4.0230385 ],\n",
              "         [-4.        ],\n",
              "         [-4.99999966],\n",
              "         [-4.00001898],\n",
              "         [-4.07351038],\n",
              "         [-4.9999911 ],\n",
              "         [-4.61055126],\n",
              "         [-4.00000116],\n",
              "         [-4.99999177],\n",
              "         [-4.3682939 ],\n",
              "         [-4.99997104],\n",
              "         [-4.99539253],\n",
              "         [-4.87349247],\n",
              "         [-4.99989651],\n",
              "         [-4.00000003],\n",
              "         [-4.98228598],\n",
              "         [-4.58398364],\n",
              "         [-4.34239805],\n",
              "         [-4.94236963],\n",
              "         [-5.        ],\n",
              "         [-4.73880737],\n",
              "         [-4.00004126],\n",
              "         [-4.55323644],\n",
              "         [-4.9973957 ],\n",
              "         [-4.99628887],\n",
              "         [-4.99924019],\n",
              "         [-4.98400837],\n",
              "         [-4.00060037],\n",
              "         [-4.99951358],\n",
              "         [-4.99890793],\n",
              "         [-4.91589667],\n",
              "         [-4.8715969 ],\n",
              "         [-4.99999971],\n",
              "         [-4.0001875 ],\n",
              "         [-4.43421923],\n",
              "         [-4.00000909],\n",
              "         [-4.06374541],\n",
              "         [-4.07640925],\n",
              "         [-4.11297928],\n",
              "         [-4.00012929],\n",
              "         [-4.15068312],\n",
              "         [-4.99860029],\n",
              "         [-4.99999886],\n",
              "         [-4.99094237],\n",
              "         [-4.00000005],\n",
              "         [-4.00166258],\n",
              "         [-4.99951656],\n",
              "         [-4.02650781],\n",
              "         [-4.51049777],\n",
              "         [-4.83772839],\n",
              "         [-4.00007189],\n",
              "         [-4.19406255],\n",
              "         [-4.00008403],\n",
              "         [-4.02060416],\n",
              "         [-4.99999998],\n",
              "         [-4.9996926 ],\n",
              "         [-4.98953777],\n",
              "         [-4.98795203],\n",
              "         [-4.00017899],\n",
              "         [-4.99764359],\n",
              "         [-4.96966667],\n",
              "         [-4.00016877],\n",
              "         [-4.99999994],\n",
              "         [-4.11842018],\n",
              "         [-4.00000915],\n",
              "         [-4.94467963],\n",
              "         [-4.00292126],\n",
              "         [-4.99997374],\n",
              "         [-4.00000016],\n",
              "         [-4.08247952],\n",
              "         [-4.99977244],\n",
              "         [-4.98457595],\n",
              "         [-4.00674642],\n",
              "         [-4.11549864],\n",
              "         [-4.72846873],\n",
              "         [-4.74373904],\n",
              "         [-4.9998736 ],\n",
              "         [-4.00000014],\n",
              "         [-4.74895978],\n",
              "         [-4.00097828],\n",
              "         [-4.99999762],\n",
              "         [-4.01873965],\n",
              "         [-4.99835549],\n",
              "         [-4.0299905 ],\n",
              "         [-4.98109236],\n",
              "         [-4.79470762],\n",
              "         [-4.99999986],\n",
              "         [-4.00000697],\n",
              "         [-4.00016133],\n",
              "         [-4.00000014],\n",
              "         [-4.00002019],\n",
              "         [-4.00000121],\n",
              "         [-4.99166598],\n",
              "         [-4.02294199],\n",
              "         [-4.9999643 ],\n",
              "         [-4.98255765],\n",
              "         [-4.52924468],\n",
              "         [-4.00002913],\n",
              "         [-4.55242105],\n",
              "         [-4.99814939],\n",
              "         [-4.00160367]]),\n",
              "  array([[-4.57639938],\n",
              "         [-4.99973956],\n",
              "         [-4.00003421],\n",
              "         [-4.02510513],\n",
              "         [-4.92168039],\n",
              "         [-4.19583478],\n",
              "         [-4.00000131],\n",
              "         [-4.52853208],\n",
              "         [-4.37689642],\n",
              "         [-4.98220913],\n",
              "         [-4.99957766],\n",
              "         [-4.0000526 ],\n",
              "         [-4.99949495],\n",
              "         [-4.99998706],\n",
              "         [-4.98426735],\n",
              "         [-4.99400173],\n",
              "         [-4.99996587],\n",
              "         [-4.00013298],\n",
              "         [-4.00000439],\n",
              "         [-4.07499525],\n",
              "         [-4.9999975 ],\n",
              "         [-4.00000743],\n",
              "         [-4.99989825],\n",
              "         [-4.49500063],\n",
              "         [-4.00000022],\n",
              "         [-4.93327731],\n",
              "         [-4.04899271],\n",
              "         [-4.99926336],\n",
              "         [-4.00139115],\n",
              "         [-4.99521498],\n",
              "         [-4.1395355 ],\n",
              "         [-4.96482861],\n",
              "         [-4.1500276 ],\n",
              "         [-4.77281378],\n",
              "         [-4.99880694],\n",
              "         [-4.99559091],\n",
              "         [-4.09484915],\n",
              "         [-4.92112333],\n",
              "         [-4.00066046],\n",
              "         [-4.82262391],\n",
              "         [-4.00000261],\n",
              "         [-4.09738885],\n",
              "         [-4.00000058],\n",
              "         [-4.99981165],\n",
              "         [-4.99930657],\n",
              "         [-4.09228635],\n",
              "         [-4.03648252],\n",
              "         [-4.04046404],\n",
              "         [-4.99999829],\n",
              "         [-4.99949488],\n",
              "         [-4.0018114 ],\n",
              "         [-4.99996012],\n",
              "         [-4.00034526],\n",
              "         [-4.00030653],\n",
              "         [-4.92037978],\n",
              "         [-4.0332235 ],\n",
              "         [-4.05347876],\n",
              "         [-4.0000006 ],\n",
              "         [-4.99999969],\n",
              "         [-4.01071444],\n",
              "         [-4.99999998],\n",
              "         [-4.99999999],\n",
              "         [-4.99736136],\n",
              "         [-4.00000303]]),\n",
              "  array([[-4.00123375],\n",
              "         [-4.75100887],\n",
              "         [-4.00000951],\n",
              "         [-4.08295811],\n",
              "         [-4.44974416],\n",
              "         [-4.73000259],\n",
              "         [-4.00123476],\n",
              "         [-4.970807  ],\n",
              "         [-4.88553997],\n",
              "         [-4.2269995 ]])],\n",
              " [array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
              "  array([[-2.94956935e-07, -2.86200726e-03, -4.57635732e+00, ...,\n",
              "          -2.04830004e+00, -8.46911410e-03, -4.56906035e+00],\n",
              "         [-3.22241949e-07, -3.12675747e-03, -4.99969361e+00, ...,\n",
              "          -2.23777819e+00, -9.25255016e-03, -4.99172164e+00],\n",
              "         [-2.57809193e-07, -2.50155767e-03, -3.99999745e+00, ...,\n",
              "          -1.79033112e+00, -7.40248902e-03, -3.99361949e+00],\n",
              "         ...,\n",
              "         [-3.22258735e-07, -3.12692034e-03, -4.99995404e+00, ...,\n",
              "          -2.23789475e+00, -9.25303212e-03, -4.99198166e+00],\n",
              "         [-3.22088670e-07, -3.12527018e-03, -4.99731543e+00, ...,\n",
              "          -2.23671376e+00, -9.24814905e-03, -4.98934726e+00],\n",
              "         [-2.57807183e-07, -2.50153817e-03, -3.99996627e+00, ...,\n",
              "          -1.79031716e+00, -7.40243131e-03, -3.99358836e+00]]),\n",
              "  array([[-1.69492511e+00, -1.04209419e-03, -4.00109685e+00,\n",
              "          -3.90078226e+00, -3.13375048e-01, -3.21765303e+00,\n",
              "          -4.00122851e+00, -1.88645335e+00, -2.49318308e+00,\n",
              "          -7.11854127e-02, -1.68988851e-03, -4.00102329e+00,\n",
              "          -2.02081171e-03, -5.17569859e-05, -6.29500222e-02,\n",
              "          -2.40004942e-02, -1.36580927e-04, -4.00070165e+00,\n",
              "          -4.00121617e+00, -3.70116024e+00, -9.99581414e-06,\n",
              "          -4.00120401e+00, -4.07118708e-04, -2.02062050e+00,\n",
              "          -4.00123285e+00, -2.66973061e-01, -3.80520248e+00,\n",
              "          -2.94746723e-03, -3.99566744e+00, -1.91459635e-02,\n",
              "          -3.44291960e+00, -1.40728957e-01, -3.40093826e+00,\n",
              "          -9.09025184e-01, -4.77369571e-03, -1.76417992e-02,\n",
              "          -3.62172012e+00, -3.15603990e-01, -3.99859108e+00,\n",
              "          -7.09723202e-01, -4.00122332e+00, -3.61155820e+00,\n",
              "          -4.00123142e+00, -7.53618572e-04, -2.77458915e-03,\n",
              "          -3.63197448e+00, -3.85525864e+00, -3.83932766e+00,\n",
              "          -6.83105682e-06, -2.02108393e-03, -3.99398591e+00,\n",
              "          -1.59555640e-04, -3.99985229e+00, -4.00000724e+00,\n",
              "          -3.18579126e-01, -3.86829875e+00, -3.78725273e+00,\n",
              "          -4.00123133e+00, -1.23813420e-06, -3.95836276e+00,\n",
              "          -9.47412119e-08, -3.60936008e-08, -1.05578135e-02,\n",
              "          -4.00122162e+00],\n",
              "         [-2.01253032e+00, -1.23736804e-03, -4.75084632e+00,\n",
              "          -4.63173419e+00, -3.72097140e-01, -3.82059612e+00,\n",
              "          -4.75100265e+00, -2.23994827e+00, -2.96037065e+00,\n",
              "          -8.45245614e-02, -2.00654993e-03, -4.75075898e+00,\n",
              "          -2.39948351e-03, -6.14555197e-05, -7.47459741e-02,\n",
              "          -2.84978504e-02, -1.62174279e-04, -4.75037707e+00,\n",
              "          -4.75098800e+00, -4.39470579e+00, -1.18688896e-05,\n",
              "          -4.75097357e+00, -4.83407048e-04, -2.39925646e+00,\n",
              "          -4.75100781e+00, -3.17000071e-01, -4.51824409e+00,\n",
              "          -3.49978127e-03, -4.74439952e+00, -2.27336488e-02,\n",
              "          -4.08807448e+00, -1.67099591e-01, -4.03822642e+00,\n",
              "          -1.07936376e+00, -5.66821938e-03, -2.09476251e-02,\n",
              "          -4.30037971e+00, -3.74743755e-01, -4.74787100e+00,\n",
              "          -8.42715382e-01, -4.75099649e+00, -4.28831359e+00,\n",
              "          -4.75100610e+00, -8.94836130e-04, -3.29450827e-03,\n",
              "          -4.31255559e+00, -4.57768008e+00, -4.55876385e+00,\n",
              "          -8.11110113e-06, -2.39980673e-03, -4.74240289e+00,\n",
              "          -1.89454130e-04, -4.74936855e+00, -4.74955253e+00,\n",
              "          -3.78276389e-01, -4.59316372e+00, -4.49693081e+00,\n",
              "          -4.75100601e+00, -1.47014320e-06, -4.70010446e+00,\n",
              "          -1.12494387e-07, -4.28570357e-08, -1.25361998e-02,\n",
              "          -4.75099448e+00],\n",
              "         [-1.69440652e+00, -1.04177535e-03, -3.99987266e+00,\n",
              "          -3.89958876e+00, -3.13279166e-01, -3.21666855e+00,\n",
              "          -4.00000427e+00, -1.88587616e+00, -2.49242025e+00,\n",
              "          -7.11636325e-02, -1.68937146e-03, -3.99979912e+00,\n",
              "          -2.02019342e-03, -5.17411501e-05, -6.29307617e-02,\n",
              "          -2.39931509e-02, -1.36539138e-04, -3.99947758e+00,\n",
              "          -3.99999194e+00, -3.70002781e+00, -9.99275577e-06,\n",
              "          -3.99997978e+00, -4.06994144e-04, -2.02000226e+00,\n",
              "          -4.00000861e+00, -2.66891377e-01, -3.80403822e+00,\n",
              "          -2.94656541e-03, -3.99444491e+00, -1.91401055e-02,\n",
              "          -3.44186619e+00, -1.40685899e-01, -3.39989769e+00,\n",
              "          -9.08747054e-01, -4.77223513e-03, -1.76364014e-02,\n",
              "          -3.62061200e+00, -3.15507427e-01, -3.99736765e+00,\n",
              "          -7.09506051e-01, -3.99999908e+00, -3.61045319e+00,\n",
              "          -4.00000718e+00, -7.53387991e-04, -2.77374022e-03,\n",
              "          -3.63086322e+00, -3.85407907e+00, -3.83815296e+00,\n",
              "          -6.82896676e-06, -2.02046555e-03, -3.99276389e+00,\n",
              "          -1.59506821e-04, -3.99862848e+00, -3.99878338e+00,\n",
              "          -3.18481652e-01, -3.86711519e+00, -3.78609396e+00,\n",
              "          -4.00000710e+00, -1.23775538e-06, -3.95715164e+00,\n",
              "          -9.47122244e-08, -3.60825574e-08, -1.05545832e-02,\n",
              "          -3.99999739e+00],\n",
              "         [-1.72954360e+00, -1.06337875e-03, -4.08281842e+00,\n",
              "          -3.98045493e+00, -3.19775668e-01, -3.28337292e+00,\n",
              "          -4.08295276e+00, -1.92498377e+00, -2.54410582e+00,\n",
              "          -7.26393599e-02, -1.72440413e-03, -4.08274335e+00,\n",
              "          -2.06208637e-03, -5.28141115e-05, -6.42357632e-02,\n",
              "          -2.44906992e-02, -1.39370564e-04, -4.08241514e+00,\n",
              "          -4.08294017e+00, -3.77675566e+00, -1.01999765e-05,\n",
              "          -4.08292777e+00, -4.15434022e-04, -2.06189125e+00,\n",
              "          -4.08295719e+00, -2.72425930e-01, -3.88292294e+00,\n",
              "          -3.00766863e-03, -4.07727811e+00, -1.95370158e-02,\n",
              "          -3.51324052e+00, -1.43603317e-01, -3.47040171e+00,\n",
              "          -9.27591833e-01, -4.87119745e-03, -1.80021293e-02,\n",
              "          -3.69569300e+00, -3.22050136e-01, -4.08026146e+00,\n",
              "          -7.24219149e-01, -4.08294746e+00, -3.68532352e+00,\n",
              "          -4.08295573e+00, -7.69011074e-04, -2.83125955e-03,\n",
              "          -3.70615680e+00, -3.93400149e+00, -3.91774512e+00,\n",
              "          -6.97057973e-06, -2.06236414e-03, -4.07556224e+00,\n",
              "          -1.62814530e-04, -4.08154844e+00, -4.08170655e+00,\n",
              "          -3.25086038e-01, -3.94730794e+00, -3.86460657e+00,\n",
              "          -4.08295565e+00, -1.26342283e-06, -4.03921149e+00,\n",
              "          -9.66762813e-08, -3.68308051e-08, -1.07734547e-02,\n",
              "          -4.08294574e+00],\n",
              "         [-1.88491440e+00, -1.15890569e-03, -4.44959192e+00,\n",
              "          -4.33803277e+00, -3.48502207e-01, -3.57832951e+00,\n",
              "          -4.44973834e+00, -2.09791162e+00, -2.77265153e+00,\n",
              "          -7.91648014e-02, -1.87931323e-03, -4.44951012e+00,\n",
              "          -2.24733062e-03, -5.75585833e-05, -7.00062810e-02,\n",
              "          -2.66907823e-02, -1.51890697e-04, -4.44915242e+00,\n",
              "          -4.44972461e+00, -4.11603450e+00, -1.11162752e-05,\n",
              "          -4.44971109e+00, -4.52753877e-04, -2.24711798e+00,\n",
              "          -4.44974317e+00, -2.96898881e-01, -4.23173915e+00,\n",
              "          -3.27785776e-03, -4.44355391e+00, -2.12920926e-02,\n",
              "          -3.82884689e+00, -1.56503692e-01, -3.78215973e+00,\n",
              "          -1.01092057e+00, -5.30879373e-03, -1.96193219e-02,\n",
              "          -4.02768970e+00, -3.50980998e-01, -4.44680527e+00,\n",
              "          -7.89278226e-01, -4.44973256e+00, -4.01638871e+00,\n",
              "          -4.44974157e+00, -8.38093961e-04, -3.08560126e-03,\n",
              "          -4.03909351e+00, -4.28740627e+00, -4.26968953e+00,\n",
              "          -7.59677068e-06, -2.24763335e-03, -4.44168389e+00,\n",
              "          -1.77440715e-04, -4.44820786e+00, -4.44838017e+00,\n",
              "          -3.54289625e-01, -4.30190808e+00, -4.21177736e+00,\n",
              "          -4.44974148e+00, -1.37692042e-06, -4.40206764e+00,\n",
              "          -1.05361041e-07, -4.01394419e-08, -1.17412708e-02,\n",
              "          -4.44973068e+00],\n",
              "         [-2.00363205e+00, -1.23189710e-03, -4.72984076e+00,\n",
              "          -4.61125528e+00, -3.70451936e-01, -3.80370360e+00,\n",
              "          -4.72999640e+00, -2.23004448e+00, -2.94728156e+00,\n",
              "          -8.41508415e-02, -1.99767810e-03, -4.72975381e+00,\n",
              "          -2.38887435e-03, -6.11837981e-05, -7.44154896e-02,\n",
              "          -2.83718490e-02, -1.61457236e-04, -4.72937358e+00,\n",
              "          -4.72998181e+00, -4.37527489e+00, -1.18164121e-05,\n",
              "          -4.72996744e+00, -4.81269695e-04, -2.38864831e+00,\n",
              "          -4.73000153e+00, -3.15598476e-01, -4.49826696e+00,\n",
              "          -3.48430722e-03, -4.72342246e+00, -2.26331334e-02,\n",
              "          -4.06999933e+00, -1.66360771e-01, -4.02037167e+00,\n",
              "          -1.07459143e+00, -5.64315772e-03, -2.08550066e-02,\n",
              "          -4.28136586e+00, -3.73086850e-01, -4.72687860e+00,\n",
              "          -8.38989371e-01, -4.72999026e+00, -4.26935309e+00,\n",
              "          -4.72999984e+00, -8.90879670e-04, -3.27994182e-03,\n",
              "          -4.29348791e+00, -4.55744016e+00, -4.53860757e+00,\n",
              "          -8.07523842e-06, -2.38919614e-03, -4.72143466e+00,\n",
              "          -1.88616471e-04, -4.72836953e+00, -4.72855269e+00,\n",
              "          -3.76603865e-01, -4.57285534e+00, -4.47704792e+00,\n",
              "          -4.72999974e+00, -1.46364305e-06, -4.67932326e+00,\n",
              "          -1.11997001e-07, -4.26675461e-08, -1.24807718e-02,\n",
              "          -4.72998826e+00],\n",
              "         [-1.69492554e+00, -1.04209446e-03, -4.00109786e+00,\n",
              "          -3.90078325e+00, -3.13375127e-01, -3.21765385e+00,\n",
              "          -4.00122952e+00, -1.88645383e+00, -2.49318371e+00,\n",
              "          -7.11854308e-02, -1.68988894e-03, -4.00102430e+00,\n",
              "          -2.02081223e-03, -5.17569990e-05, -6.29500382e-02,\n",
              "          -2.40005002e-02, -1.36580962e-04, -4.00070266e+00,\n",
              "          -4.00121718e+00, -3.70116117e+00, -9.99581667e-06,\n",
              "          -4.00120503e+00, -4.07118811e-04, -2.02062101e+00,\n",
              "          -4.00123386e+00, -2.66973129e-01, -3.80520344e+00,\n",
              "          -2.94746797e-03, -3.99566845e+00, -1.91459684e-02,\n",
              "          -3.44292048e+00, -1.40728993e-01, -3.40093912e+00,\n",
              "          -9.09025414e-01, -4.77369692e-03, -1.76418037e-02,\n",
              "          -3.62172104e+00, -3.15604070e-01, -3.99859209e+00,\n",
              "          -7.09723381e-01, -4.00122433e+00, -3.61155912e+00,\n",
              "          -4.00123243e+00, -7.53618763e-04, -2.77458985e-03,\n",
              "          -3.63197540e+00, -3.85525962e+00, -3.83932863e+00,\n",
              "          -6.83105855e-06, -2.02108444e-03, -3.99398692e+00,\n",
              "          -1.59555680e-04, -3.99985331e+00, -4.00000825e+00,\n",
              "          -3.18579207e-01, -3.86829973e+00, -3.78725369e+00,\n",
              "          -4.00123235e+00, -1.23813451e-06, -3.95836376e+00,\n",
              "          -9.47412359e-08, -3.60936099e-08, -1.05578162e-02,\n",
              "          -4.00122264e+00],\n",
              "         [-2.10563695e+00, -1.29461297e-03, -4.97063694e+00,\n",
              "          -4.84601426e+00, -3.89311643e-01, -3.99735013e+00,\n",
              "          -4.97080050e+00, -2.34357604e+00, -3.09732765e+00,\n",
              "          -8.84349605e-02, -2.09937988e-03, -4.97054555e+00,\n",
              "          -2.51049193e-03, -6.42986650e-05, -7.82039819e-02,\n",
              "          -2.98162597e-02, -1.69677023e-04, -4.97014597e+00,\n",
              "          -4.97078517e+00, -4.59802011e+00, -1.24179856e-05,\n",
              "          -4.97077007e+00, -5.05771133e-04, -2.51025438e+00,\n",
              "          -4.97080589e+00, -3.31665593e-01, -4.72727372e+00,\n",
              "          -3.66169328e-03, -4.96389188e+00, -2.37853861e-02,\n",
              "          -4.27720297e+00, -1.74830198e-01, -4.22504877e+00,\n",
              "          -1.12929887e+00, -5.93045085e-03, -2.19167349e-02,\n",
              "          -4.49933018e+00, -3.92080699e-01, -4.96752397e+00,\n",
              "          -8.81702316e-01, -4.97079405e+00, -4.48670584e+00,\n",
              "          -4.97080411e+00, -9.36234350e-04, -3.44692364e-03,\n",
              "          -4.51206936e+00, -4.78945942e+00, -4.76966806e+00,\n",
              "          -8.48634878e-06, -2.51083010e-03, -4.96180288e+00,\n",
              "          -1.98218935e-04, -4.96909080e+00, -4.96928329e+00,\n",
              "          -3.95776766e-01, -4.80565939e+00, -4.70497441e+00,\n",
              "          -4.97080401e+00, -1.53815712e-06, -4.91754758e+00,\n",
              "          -1.17698767e-07, -4.48397507e-08, -1.31161679e-02,\n",
              "          -4.97079194e+00],\n",
              "         [-2.06951778e+00, -1.27240575e-03, -4.88537282e+00,\n",
              "          -4.76288787e+00, -3.82633563e-01, -3.92878135e+00,\n",
              "          -4.88553358e+00, -2.30337537e+00, -3.04419746e+00,\n",
              "          -8.69179862e-02, -2.06336805e-03, -4.88528301e+00,\n",
              "          -2.46742806e-03, -6.31957141e-05, -7.68625053e-02,\n",
              "          -2.93048047e-02, -1.66766458e-04, -4.88489028e+00,\n",
              "          -4.88551851e+00, -4.51914770e+00, -1.22049729e-05,\n",
              "          -4.88550367e+00, -4.97095358e-04, -2.46719459e+00,\n",
              "          -4.88553888e+00, -3.25976348e-01, -4.64618415e+00,\n",
              "          -3.59888221e-03, -4.87874347e+00, -2.33773821e-02,\n",
              "          -4.20383372e+00, -1.71831237e-01, -4.15257415e+00,\n",
              "          -1.10992738e+00, -5.82872252e-03, -2.15407849e-02,\n",
              "          -4.42215066e+00, -3.85355120e-01, -4.88231326e+00,\n",
              "          -8.66577984e-01, -4.88552724e+00, -4.40974287e+00,\n",
              "          -4.88553713e+00, -9.20174599e-04, -3.38779663e-03,\n",
              "          -4.43467131e+00, -4.70730315e+00, -4.68785128e+00,\n",
              "          -8.34077770e-06, -2.46776043e-03, -4.87669031e+00,\n",
              "          -1.94818775e-04, -4.88385321e+00, -4.88404240e+00,\n",
              "          -3.88987786e-01, -4.72322523e+00, -4.62426736e+00,\n",
              "          -4.88553703e+00, -1.51177225e-06, -4.83319414e+00,\n",
              "          -1.15679815e-07, -4.40705894e-08, -1.28911789e-02,\n",
              "          -4.88552517e+00],\n",
              "         [-1.79055963e+00, -1.10089335e-03, -4.22685488e+00,\n",
              "          -4.12088014e+00, -3.31056933e-01, -3.39920600e+00,\n",
              "          -4.22699397e+00, -1.99289466e+00, -2.63385853e+00,\n",
              "          -7.52019810e-02, -1.78523884e-03, -4.22677717e+00,\n",
              "          -2.13483407e-03, -5.46773240e-05, -6.65019165e-02,\n",
              "          -2.53546989e-02, -1.44287374e-04, -4.22643738e+00,\n",
              "          -4.22698093e+00, -3.90999464e+00, -1.05598183e-05,\n",
              "          -4.22696809e+00, -4.30089988e-04, -2.13463207e+00,\n",
              "          -4.22699856e+00, -2.82036759e-01, -4.01990736e+00,\n",
              "          -3.11377522e-03, -4.22111912e+00, -2.02262561e-02,\n",
              "          -3.63718302e+00, -1.48669453e-01, -3.59283292e+00,\n",
              "          -9.60316053e-01, -5.04304689e-03, -1.86372207e-02,\n",
              "          -3.82607219e+00, -3.33411641e-01, -4.22420772e+00,\n",
              "          -7.49768649e-01, -4.22698848e+00, -3.81533689e+00,\n",
              "          -4.22699704e+00, -7.96140773e-04, -2.93114267e-03,\n",
              "          -3.83690514e+00, -4.07278789e+00, -4.05595802e+00,\n",
              "          -7.21649262e-06, -2.13512164e-03, -4.21934271e+00,\n",
              "          -1.68558413e-04, -4.22554010e+00, -4.22570379e+00,\n",
              "          -3.36554646e-01, -4.08656378e+00, -4.00094481e+00,\n",
              "          -4.22699695e+00, -1.30799473e-06, -4.18170956e+00,\n",
              "          -1.00086893e-07, -3.81301474e-08, -1.11535280e-02,\n",
              "          -4.22698670e+00]])])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net=Network([784,128,64,10])\n",
        "nabla_b,nabla_w=net.backpropagation(train_X[0],train_y[0])\n",
        "for nb in nabla_b:\n",
        "  print(nb.shape)\n",
        "for nw in nabla_w:\n",
        "  print(nw.shape)\n",
        "\n",
        "# Desired output:\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pq4E3rHik-f",
        "outputId": "413e30e2-b999-49dc-fcda-98f886133cdd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 1)\n",
            "(64, 1)\n",
            "(10, 1)\n",
            "(128, 784)\n",
            "(64, 128)\n",
            "(10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXljiAYRlvdq",
        "outputId": "fbff0ad9-0689-4b6c-bb0e-5bbf8e31c010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001/10000 classified correctly.\n",
            "Epoch 1 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 2 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 3 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 4 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 5 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 6 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 7 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 8 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 9 completed.\n",
            "1001/10000 classified correctly.\n",
            "Epoch 10 completed.\n",
            "Test data:\n",
            "85/1000 classified correctly.\n"
          ]
        }
      ],
      "source": [
        "net=Network([784,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=10,mini_batch_size=20,lr=0.01)\n",
        "print(\"Test data:\")\n",
        "net.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of question 1."
      ],
      "metadata": {
        "id": "mhMIoFT9m7OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2 :\n",
        "## Stochastic Gradient Descent\n",
        "Implement logistic regression using \"Stochastic gradient descent\" and use iris-dataset as training data.\n",
        "\n",
        "\n",
        "The word 'stochastic' means a system or process linked with a random probability. Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called “batch” which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Although using the whole dataset is really useful for getting to the minima in a less noisy and less random manner, the problem arises when our dataset gets big.\n",
        "Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima are reached. Hence, it becomes computationally very expensive to perform.\n",
        "This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.\n",
        "\n",
        "    Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm used for optimizing machine learning models. In this variant, only one random training example is used to calculate the gradient and update the parameters at each iteration. Here are some of the advantages and disadvantages of using SGD:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Speed: SGD is faster than other variants of Gradient Descent such as Batch Gradient Descent and Mini-Batch Gradient Descent since it uses only one example to update the parameters.\n",
        "\n",
        "Memory Efficiency: Since SGD updates the parameters for each training example one at a time, it is memory-efficient and can handle large datasets that cannot fit into memory.\n",
        "\n",
        "Avoidance of Local Minima: Due to the noisy updates in SGD, it has the ability to escape from local minima and converge to a global minimum.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Noisy updates: The updates in SGD are noisy and have a high variance, which can make the optimization process less stable and lead to oscillations around the minimum.\n",
        "\n",
        "Slow Convergence: SGD may require more iterations to converge to the minimum since it updates the parameters for each training example one at a time.\n",
        "\n",
        "Sensitivity to Learning Rate: The choice of learning rate can be critical in SGD since using a high learning rate can cause the algorithm to overshoot the minimum, while a low learning rate can make the algorithm converge slowly.\n",
        "\n",
        "Less Accurate: Due to the noisy updates, SGD may not converge to the exact global minimum and can result in a suboptimal solution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "So, in SGD, we find out the gradient of the cost function of a single example at each iteration instead of the sum of the gradient of the cost function of all the examples.\n",
        "\n",
        "In SGD, since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than your typical Gradient Descent algorithm. But that doesn’t matter all that much because the path taken by the algorithm does not matter, as long as we reach the minima and with a significantly shorter training time."
      ],
      "metadata": {
        "id": "Aa_iPRK6nEay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "# pre load sklearn iris datasets\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "\n",
        "dataset = []\n",
        "\n",
        "\n",
        "target_label = 0 # choose the target label of flower type\n",
        "for index, x in enumerate(X):\n",
        "    transform_label = None\n",
        "    if Y[index]==target_label:\n",
        "      transform_label=0\n",
        "    else:\n",
        "        transform_label=1\n",
        "    x = [x[0], x[2]]\n",
        "    dataset.append((x,transform_label))\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sgd(dataset, w):\n",
        "    #run sgd randomly\n",
        "    index = random.randint(0, len(dataset) - 1)\n",
        "    data=np.array(dataset[index][0])\n",
        "    data= np.reshape(data,(1,2))\n",
        "    y_p  = np.array(dataset[index][1])\n",
        "    y_p= np.reshape(y_p,(1,1))\n",
        "    #print(data.shape,\"  \",w.T.shape)\n",
        "    #print(\"sgd\",np.dot(data,w.T).shape)\n",
        "    return sigmoid(np.dot(data,w.T)),y_p,data\n",
        "def cost(dataset, w):\n",
        "    total_cost = 0\n",
        "    for x,y in dataset:\n",
        "      y_pred = np.dot(x,w.T)\n",
        "      total_cost+=0.5*(y_pred-y)**2\n",
        "    return total_cost/len(dataset)\n",
        "\n",
        "def logistic_regression(dataset):\n",
        "    w = np.random.rand(1,2)\n",
        "    limit = 1500 #update times\n",
        "    eta = 0.1 #update rate\n",
        "    costs = []\n",
        "    for i in range(limit):\n",
        "        a,y,x = sgd(dataset,w)\n",
        "        #print(\"a\",a.shape)\n",
        "        a= np.reshape(a,(1,1))\n",
        "        x=np.reshape(x,(1,2))\n",
        "        y= np.array(y)\n",
        "        dz = a-y.astype(float)\n",
        "        dz= np.reshape(dz,(1,1))\n",
        "        dw =np.dot(x.T,dz)\n",
        "        dw=dw.T\n",
        "        #print(\"dw \",dw.shape)\n",
        "        costs.append(cost(dataset,w))\n",
        "        w =w-eta*dw\n",
        "        #print(w.shape)\n",
        "        eta = eta * 0.98 #decrease update rate\n",
        "        #print(\"iteration done\")\n",
        "    plt.plot(range(limit), costs)\n",
        "    plt.show()\n",
        "    return w,(limit, costs)\n",
        "\n",
        "def main():\n",
        "    #execute\n",
        "    w,(lim,coshs) = logistic_regression(dataset)\n",
        "    #draw\n",
        "    ps = [v[0] for v in dataset]\n",
        "    label = [v[1] for v in dataset]\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    #plot via label\n",
        "    tpx=[]\n",
        "    for index, label_value in enumerate(label):\n",
        "        px=ps[index][0]\n",
        "        py=ps[index][1]\n",
        "        tpx.append(px)\n",
        "        if label_value == 1:\n",
        "            ax1.scatter(px, py, c='b', marker=\"o\", label='O')\n",
        "        else:\n",
        "            ax1.scatter(px, py, c='r', marker=\"x\", label='X')\n",
        "\n",
        "    l = np.linspace(min(tpx),max(tpx))\n",
        "    a =-w[0][0]/w[0][1]\n",
        "    b=w[0][1]\n",
        "    ax1.plot(l, a*l + b, 'g-')\n",
        "    #plt.legend(loc='upper left');\n",
        "    plt.show()\n",
        "\n",
        "    # limit = w[1][0]\n",
        "    # costs = w[1][1]\n",
        "    # w = w[0]\n",
        "\n",
        "    # calculate score\n",
        "    predicted_Y=[]\n",
        "    answer_Y=[]\n",
        "    # for X,Y in dataset:\n",
        "    #     prediction, (ll,cc)= sgd([(X, Y)], w)\n",
        "    #     predicted_Y.append(prediction)\n",
        "    #     answer_Y.append(Y)\n",
        "    # predicted_Y = np.asarray(predicted_Y)\n",
        "    # predicted_Y = predicted_Y > 0.5\n",
        "    # print(answer_Y)\n",
        "    # print(predicted_Y)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RMgp1wALns9d",
        "outputId": "a2353393-0318-4e02-d9c0-30b5f9ff7b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-4f604530aa4e>:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  dataset = np.array(dataset)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoZElEQVR4nO3de3xU5b3v8e/MJJkkkBtgEgIJREtFARG5iait22yR4q3d21Y3Uqr7Vbcai1Q3AqcFt9ti0Pa0tNXi5ZwqfVVFPUfxchQ2xQtSwy3cxAvgBiGCISJkJgEyucxz/ggMGRJCEtaslcz6vF+veTGz1pNZzy+EzJdnPetZHmOMEQAAgE28TncAAAC4C+EDAADYivABAABsRfgAAAC2InwAAABbET4AAICtCB8AAMBWhA8AAGCrBKc7cLJwOKx9+/YpLS1NHo/H6e4AAIB2MMaourpaeXl58nrbHtvocuFj3759ys/Pd7obAACgE8rLy9W/f/8223S58JGWliapqfPp6ekO9wYAALRHMBhUfn5+5HO8LV0ufBw/1ZKenk74AACgm2nPlIkOTzhduXKlrr32WuXl5cnj8WjJkiVR+40xmjt3rvr27auUlBQVFRVpx44dHT0MAACIUx0OH4cPH9bw4cP1+OOPt7r/0Ucf1R/+8Ac98cQTWrNmjXr06KEJEyaotrb2jDsLAAC6vw6fdpk4caImTpzY6j5jjBYsWKBf/vKXuv766yVJf/nLX5STk6MlS5bopptuOrPeAgCAbs/SdT527dqliooKFRUVRbZlZGRo7NixKi0ttfJQAACgm7J0wmlFRYUkKScnJ2p7Tk5OZN/JQqGQQqFQ5HUwGLSySwAAoItxfIXTkpISZWRkRB6s8QEAQHyzNHzk5uZKkvbv3x+1ff/+/ZF9J5s9e7YCgUDkUV5ebmWXAABAF2Np+CgsLFRubq5WrFgR2RYMBrVmzRqNGzeu1a/x+/2RNT1Y2wMAgPjX4TkfNTU1+vzzzyOvd+3apU2bNqlXr14qKCjQ9OnT9atf/UqDBg1SYWGh5syZo7y8PN1www1W9hsAAHRTHQ4f69ev1xVXXBF5fe+990qSpk6dqmeffVb333+/Dh8+rNtvv11VVVW69NJLtXTpUiUnJ1vXawAA0G15jDHG6U40FwwGlZGRoUAgwCkYAAC6iY58fjt+tQsAAHAXV4UPY4ye/fsubdxzyOmuAADgWl3urrax9MaWr/Qfb3wiSfpi/iSHewMAgDu5auRj054qp7sAAIDruSp81ITqne4CAACu56rwUV3b4HQXAABwPVeFj5oQ4QMAAKe5Knww8gEAgPNcFj6Y8wEAgNNcFT447QIAgPPcFT447QIAgONcFT4O1zU63QUAAFzPVeEDAAA4j/ABAABsRfgAAAC2ck34CIdN5Hma31X30wMAoEtxTfg4Un9ismnPZMIHAABOcU34aL7AWHKiz8GeAADgbi4KHyfW+PA42A8AANzONeGjb0ay7vjOOZIkc5q2AAAgdlwTPtKSE1V0XrbT3QAAwPVcEz4AAEDXQPgAAAC2cmX4MIZZHwAAOMVV4cPDZS4AADjOVeEDAAA4j/ABAABs5crwwYwPAACc47LwwaQPAACc5rLwAQAAnEb4AAAAtnJl+GCZDwAAnOOq8ME6HwAAOM9V4QMAADiP8AEAAGxF+AAAALZyZfgwLDMGAIBjXBU+mG8KAIDzXBU+AACA8wgfAADAVq4MHywyBgCAc1wVPjysMgYAgONcFT4AAIDzCB8AAMBWrgwfzPkAAMA5rgofzPgAAMB5rgofAADAeYQPAABgK8IHAACwlavCB8t8AADgPFeFDwAA4DzCBwAAsJUrw4dhoQ8AABzjqvDhYaUPAAAc56rwAQAAnEf4AAAAtiJ8AAAAW7kyfDDdFAAA57gqfLDIGAAAzrM8fDQ2NmrOnDkqLCxUSkqKzjnnHD300ENc3goAACRJCVa/4SOPPKKFCxdq0aJFGjJkiNavX69bb71VGRkZmjZtmtWHAwAA3Yzl4ePDDz/U9ddfr0mTJkmSBg4cqBdeeEFr1661+lCdxiAMAADOsfy0yyWXXKIVK1Zo+/btkqTNmzdr1apVmjhxYqvtQ6GQgsFg1AMAAMQvy0c+Zs2apWAwqMGDB8vn86mxsVHz5s3T5MmTW21fUlKiBx980OpuAACALsrykY+XXnpJzz33nJ5//nlt2LBBixYt0m9+8xstWrSo1fazZ89WIBCIPMrLy63uEgAA6EIsH/mYMWOGZs2apZtuukmSNGzYMO3evVslJSWaOnVqi/Z+v19+v9/qbrTJsNIHAACOsXzk48iRI/J6o9/W5/MpHA5bfagOY50PAACcZ/nIx7XXXqt58+apoKBAQ4YM0caNG/Xb3/5Wt912m9WHAgAA3ZDl4eOPf/yj5syZo7vuukuVlZXKy8vTv/3bv2nu3LlWHwoAAHRDloePtLQ0LViwQAsWLLD6rS3DOh8AADjHXfd2EZM+AABwmqvCBwAAcB7hAwAA2MqV4YMpHwAAOMdV4YN1PgAAcJ6rwgcAAHAe4QMAANiK8AEAAGzlyvDBImMAADjHVeGDCacAADjPVeEDAAA4j/ABAABs5dLwwaQPAACc4qrwwY3lAABwnqvCBwAAcB7hAwAA2MqV4YN1PgAAcI6rwgfrfAAA4DxXhQ8AAOA8wgcAALCVK8MHUz4AAHCOq8IHUz4AAHCeq8IHAABwHuEDAADYypXhw7DQBwAAjnFV+GCdDwAAnOeq8AEAAJxH+AAAALYifAAAAFu5Mnww3RQAAOe4LHww4xQAAKe5LHwAAACnET4AAICtXBk+WGMMAADnuCp8sMgYAADOc1X4AAAAziN8AAAAW7kyfHBjOQAAnOOq8MGUDwAAnOeq8AEAAJxH+AAAALZyZfhgxgcAAM5xVfjwsNAHAACOc1X4AAAAziN8AAAAW7kzfDDpAwAAx7gqfDDjAwAA57kqfAAAAOcRPgAAgK0IHwAAwFauDB/MNwUAwDmuCh+sMQYAgPNcFT4AAIDzCB8AAMBWrgwfxjDrAwAAp7gqfHhYZgwAAMe5KnwAAADnET4AAICtXBk+mPEBAIBzYhI+9u7dq1tuuUW9e/dWSkqKhg0bpvXr18fiUB3COh8AADgvweo3PHTokMaPH68rrrhCb7/9ts466yzt2LFDWVlZVh8KAAB0Q5aHj0ceeUT5+fl65plnItsKCwutPgwAAOimLD/t8vrrr2vUqFG68cYblZ2drREjRujpp58+ZftQKKRgMBj1iDWW+QAAwDmWh4+dO3dq4cKFGjRokJYtW6Y777xT06ZN06JFi1ptX1JSooyMjMgjPz/f6i4BAIAuxGMsXu4zKSlJo0aN0ocffhjZNm3aNK1bt06lpaUt2odCIYVCocjrYDCo/Px8BQIBpaenW9k1lR88ossefVcpiT59+tDVlr43AABuFgwGlZGR0a7Pb8tHPvr27avzzz8/att5552nPXv2tNre7/crPT096gEAAOKX5eFj/Pjx2rZtW9S27du3a8CAAVYfqtMMK30AAOAYy8PHz3/+c61evVoPP/ywPv/8cz3//PN66qmnVFxcbPWhOox1PgAAcJ7l4WP06NF69dVX9cILL2jo0KF66KGHtGDBAk2ePNnqQwEAgG7I8nU+JOmaa67RNddcE4u3BgAA3Zwr7+0CAACc48rwwSJjAAA4x1Xhw8OMUwAAHOeq8AEAAJxH+AAAALZyZfhgygcAAM5xVfhgxgcAAM5zVfgAAADOI3wAAABbuTN8MOkDAADHuCp8sMwHAADOc1X4AAAAziN8AAAAW7kyfBgmfQAA4BhXhQ8PK30AAOA4V4UPAADgPMIHAACwlSvDh2HKBwAAjnFV+GCdDwAAnOeq8AEAAJxH+AAAALZyZfhgygcAAM5xVfhgygcAAM5zVfgAAADOI3wAAABbET4AAICtXBk+DKuMAQDgGHeFD2acAgDgOHeFDwAA4DjCBwAAsJUrwwczPgAAcI6rwoeHSR8AADjOVeEDAAA4j/ABAABs5crwwTIfAAA4x1Xhw8OUDwAAHOeq8AEAAJxH+AAAALYifAAAAFu5Knww5QMAAOe5KnwAAADnET4AAICtXBs+Xl5frq+rQ053AwAA13FV+PA0W+hjxv/Zoh8+WepgbwAAcCdXhY+T7Tpw2OkuAADgOq4OHwAAwH6EDwAAYCvCBwAAsJWrwgeLjAEA4DxXhQ8AAOA8wgcAALAV4QMAANjKVeHDw6QPAAAc56rwAQAAnEf4AAAAtiJ8AAAAW7kqfHhY6QMAAMe5KnwAAADnxTx8zJ8/Xx6PR9OnT4/1oQAAQDcQ0/Cxbt06Pfnkk7rgggtieRgAANCNxCx81NTUaPLkyXr66aeVlZUVq8N0DFM+AABwXMzCR3FxsSZNmqSioqI224VCIQWDwagHAACIXwmxeNPFixdrw4YNWrdu3WnblpSU6MEHH4xFNwAAQBdk+chHeXm57rnnHj333HNKTk4+bfvZs2crEAhEHuXl5VZ3CQAAdCGWj3yUlZWpsrJSF110UWRbY2OjVq5cqccee0yhUEg+ny+yz+/3y+/3W92NVnFvFwAAnGd5+Ljyyiv10UcfRW279dZbNXjwYM2cOTMqeAAAAPexPHykpaVp6NChUdt69Oih3r17t9gOAADchxVOAQCArWJytcvJ3nvvPTsOAwAAugFXjXww3xQAAOe5KnwAAADnET4AAICtCB8AAMBWrgofHlYZAwDAca4KHwAAwHmEDwAAYCvCBwAAsJWrwgczPgAAcJ6rwgcAAHAe4QMAANiK8AEAAGzlqvDBMh8AADjPVeEDAAA4j/ABAABsRfgAAAC2clX48LDSBwAAjnNV+AAAAM4jfAAAAFsRPgAAgK0IHwAAwFauCh8sMgYAgPNcFT4AAIDzCB8AAMBWhA8AAGArwgcAALAV4QMAANiK8AEAAGxF+AAAALZyVfhgnQ8AAJznqvABAACcR/gAAAC2InwAAABbuSp8eMSkDwAAnOaq8AEAAJxH+AAAALYifAAAAFu5KnywzgcAAM5zVfgAAADOI3wAAABbET4AAICtCB8AAMBWrgofzDcFAMB5rgofAADAeYQPAABgK8IHAACwlavCh4dVxgAAcJyrwgcAAHAe4QMAANiK8AEAAGzlqvBxpjM+PvoyoLc++sqSvgAA4FYJTnegO7n2sVWSpLemXabz89Id7g0AAN2Tq0Y+zkRj2ESeV1bXOtgTAAC6N8JHO1UdqYs8z0xNcrAnAAB0b64KH2eyzMeBmhPhw8d6IQAAdJqrwsfp7P7msH74ZKne/ayyxb5gbX3kediYFvsBAED7ED6auWfxJq3ddVC3Pruuxb4jdY2R54QPAAA6j/DRTEXg1BNJj9Y1RJ6HyR4AAHSaq8LH6e7t0hAOn3Lf4dCJkQ/DyAcAAJ1mefgoKSnR6NGjlZaWpuzsbN1www3atm2b1YeJiVDDqcPHkfrmp13s6A0AAPHJ8vDx/vvvq7i4WKtXr9by5ctVX1+vq666SocPH7b6UJarb2wjfISan3YhfQAA0FmWr3C6dOnSqNfPPvussrOzVVZWpssvv9zqw1mqvvHUoaL5hFOyBwAAnRfz5dUDgYAkqVevXq3uD4VCCoVCkdfBYDDWXTqlxjbOp9Q1GxVhzgcAAJ0X0wmn4XBY06dP1/jx4zV06NBW25SUlCgjIyPyyM/Pj2WXOq2+2XwQ5nwAANB5MQ0fxcXF2rp1qxYvXnzKNrNnz1YgEIg8ysvLY9mlTms+H4Q5HwAAdF7MTrvcfffdevPNN7Vy5Ur179//lO38fr/8fn+sumGZumbzQQgfAAB0nuXhwxijn/3sZ3r11Vf13nvvqbCw0OpDOKI+as6Hgx0BAKCbszx8FBcX6/nnn9drr72mtLQ0VVRUSJIyMjKUkpJi9eFs08BpFwAALGH5nI+FCxcqEAjou9/9rvr27Rt5vPjii1Yfylb1UaddHOwIAADdXExOu3R3ra3CXsfIBwAAlnDVvV3aK8nX8ttSzzofAABYIuaLjHVHzcOHMUZ/+7RS5QePRLZx2gUAgM4jfLQiMaEpfLy5ZZ/e3/a1Xi77Mmo/Ax8AAHQe4aMViT6Pdn9zWHc/v7HV/cz5AACg85jz0YpEn1d7q46ecj/hAwCAziN8tOLLQ0f17meVp9xP9gAAoPMIH8c0X0RMkp7+YNcp2zLyAQBA5xE+jqk7KXy0hatdAADoPMLHMXUNHQkfpA8AADrL9eEjcKReUsfCB4uMAQDQea4PH2NL/iZJCnVo5CNWvQEAIP65PnzU1jeFjo6FD9IHAACd5frwcVzHTrvEsCMAAMQ5wscxoYbGdrdlzgcAAJ1H+DimY1e7xLAjAADEOcLHMR1b54P0AQBAZxE+jgnVtz98lP73NzHsCQAA8Y3wcUxHRj7+65P9MewJAADxjfBxzJG69k84BQAAnUf4kLR9f7WO1jU43Q0AAFyB8CFp2gsbGfkAAMAmhA9J1bUNhA8AAGxC+JDk83p0tP7Mwkd9Y1hLNu5VRaDWol4BABCfCB+SvB7pSDvmfAzJS5ckFfRKbbHvqZU7Nf3FTbrusVWW9w8AgHhC+JDk9Xh0tO70l9r28CdIkhpbWeJ02ccVkqTK6pC1nQMAIM4QPiTJIzWETx8+erYRPhoaWfUUAID2SHC6A12CkRraccOW4yMfFcFaPfjGx/rxuIEq7NNDUuuB5Lhw2GjDnkM6UFOnq4fmWtNni1QdqdNj73yuvVVHNWpgLxljlJqUoMsG9VF+r1Q1ho02f1mlr6pq9VXgqHIzktU/K1W7DtRoc3lAklTYp4dSEn3qm5msgb17KDnRp6QEr3Z+XaODh+tUWx+Wz+tRfWNYiT6P6hqNErweNYaNao/NtUlK8Co50adEn0cJXq98Xo8SvB4l+LxK9HmOvW7a7vN65PN45PU2jVr5vB55PJJM0313wsYobIxM5PmJmwF6PJ6mPyUdeyop+k7Fzf8mY3kTQeIqAKck+bzKb2UKgV0IH8c0tGOF055+X+T5M3//Qsu2VujD2Vc2fX2zkZNN5VW6MD8z8vrfX96sVzbulSS9fMc4jR7Yy6Jed0xDY1jVtQ1KS06Q1+PRn/++S79bvl2Hj13p8/bWiqj2A3un6otvjjjRVQBADJ19Vg+9c993HTs+4UPSzgOHtfPA4dO265EU/e3a1+zKluYjH//y9Gp98p9XS2q6CuZ48JCkLV8GbAsfxhgFjtYrMzVJn1fW6LZn12nPwZZhol9milKSfNr9zeFjoxpN34vmwWNQdk9lpSZp54Ea1daHlZPu14X5WfJ6pF0HDstIqqyu1YHqOoUaGhU2Uk66X9lpyUpO9Kq+0SjJ51XYGCX4mkY9vB6PkhN98nia7ipcW9+ohrBRQ6NRY9ioPhxWfWNYjY1G9WGjcNio4difjaapzfHRjUbT9H5eT9PohkeKvPZ6PE1DHVJkuMEc+/54mg1/NBsIiXoRtR0A4kCa39mPf8JHB6S28ZdV32zOR/M1Q/YHY3vpbWWwVmel+bW36qjyMlLk9XoUOFqvW/7XGn20N9Dm1/oTvJo9cbBuuXiAEnwnpv8YY7SpvEo79tdoQO9UDc/PVHKir413ihZqaFRDo4mcpgIAoDk+HTogNenUH8CtXaobOFLfYt2PcDvmlrTXM3/fpQff+CTyun9Wir48dPSU7fMykjX54gHaW3VUPf0JunFkfw3KSWvRzuPxaERBlkYUZHWqX/4En8gdAIBT4SOiA1Ja+d//vS9t0v+8cbgOHamPbEtLTlBlsFZjHl7Ron3gaH2LbZ1REaiNCh6SWgSP/F4pGtI3Q0s/rtD3R/TTv084V/0yUyw5PgAAnUX46ICUVkY+XtmwV3df8a2obb17JOmNLV+1+h6HjtSdcT8aw0YPvL418trjkUbkZ2rDnqrItsf+ZYSuuSDvjI8FAIDVCB8dcKrTLiffFyarR5IqAq2f/qg6cuqRj2BtvQJH6pWd7terG/bqHwZnKzs9OaqNMUYX/ud/qbq26TTPX24bo4JeqcrLTNE7n1Xq8m/3UWoSf60AgK6LT6kOOFX4aC1Q7Dtprkefnkk6UFOn/cFa7f7msAb07hG1/5uakEb+6m9K9Hk0tF+GNu6p0qQL+urxf7ko0ubhtz7VUyt3Rl7/cFR/Xf7tsyKvu9oaIgAAtIYVTjvgVFd8nHxFy8Y9VfrypEtazzmrpyRp/e5D+s6v31PZ7kNR+3/01GpJTVfNbDx2+uT/bflKXwWO6sV1e/TQm59EBY8eST795/VDz6geAACcwMhHB5zqdMbXNS3v57L5y+jLXIf1y9CaXQcjr59bvVsjBzRdTfJNTUifV9a0+t7jSt5pdfu7M77boctfAQDoKggfHTDgFEvRfnFsUS6PJ3qZ7uaGN1vxVJKSm53C+b8bvmx3H564ZaTGFvZSVo+kdn8NAABdCadd2uF7w3K1ae4/KtXf+kjD4nXlktTqZaxpyU35rvncDEk6HGqaMFpb36iH3/pMkjQ4N02XnNNbfTOS9cpdl7R4r38e2V9XD80leAAAujVGPtohKzVJmalJbd48TpKmjhuoeW99GnntT/Dqw1n/oLqGsDJSEqPafrwvKEl6aX15ZNu3c9L0ux9d2LQ0uNej/374eyo/eEQD+0RPTgUAoDsjfLRD4rGlx33etu/yMSQvXenJCQoeuwy2X2aK0pITW237eWWNlm6t0NzXPo5se+Da86OO4fN6CB4AgLjDaZd2SGgWCIb2Sz9lu76ZKZHgIUl5J52GeeWuS/TPI/tHXt/x17LI81kTB6t3T78V3QUAoEsjfLTD4L4nAsepJpRKUmZKov40+cS6HL1OmptxUUGWfnPjcA3vnxG1vUeST7dfdrY1nQUAoIsjfJzG//jeYP1gRL92tU1PSdQ/DM6OvD7VaZr//ZPRUa+nXjJQ3tOc0gEAIF4QPtqQ5PPq9svPaXcw8Hk9UWtvBE9xE7k+Pf3aOOcfdWF+pkYUZOqnjHoAAFyECadt8J4mmp1zVg/999dNa3wcXzCsueMTVVuT1SNJS4rHn1H/AADojhj5aIPX03LEo/mcjyenjIo8f/rHJ54v+NGFGpybpvuvPjem/QMAoDtyXfiYduWgdrf1tRI+msvLbLrjrMcjZaWeuKT2hhH9tHT65Tr72P1cAADACa4LH7dcXNDutq1lj4d/MExJCV7NnjhYqUkJWv/LIm2ae5U8pwkqAACgievmfHjU/pBw6aA+LbZdmJ+pjx+cEJnP0Ye1OQAA6BDXhY/2+vervq0pFw9sdV9bE0kBAEDbCB8nmTHhXPXNSNYPLup/+sYAAKDDXBc+MlNbv9fKccVXfMumngAA4E6uO3+Q6PNq8wNXOd0NAABcy3XhQ1KL29sDAAD7uDJ8SFIht6oHAMARrg0ff7ltjNNdAADAlVwbPvJ7per6C/Oc7gYAAK4Ts/Dx+OOPa+DAgUpOTtbYsWO1du3aWB2q0xrC5vSNAACApWISPl588UXde++9euCBB7RhwwYNHz5cEyZMUGVlZSwO12kjC6LvRDt74mCHegIAgHt4jDGW//d/7NixGj16tB577DFJUjgcVn5+vn72s59p1qxZbX5tMBhURkaGAoGA0tPTre5alPrGsF5aX65xZ/dWekoiS6UDANBJHfn8tnzko66uTmVlZSoqKjpxEK9XRUVFKi0ttfpwZyTR59XksQN09lk9CR4AANjE8hVODxw4oMbGRuXk5ERtz8nJ0WeffdaifSgUUigUirwOBoNWdwkAAHQhjl/tUlJSooyMjMgjPz/f6S4BAIAYsjx89OnTRz6fT/v374/avn//fuXm5rZoP3v2bAUCgcijvLzc6i4BAIAuxPLwkZSUpJEjR2rFihWRbeFwWCtWrNC4ceNatPf7/UpPT496AACA+BWTu9ree++9mjp1qkaNGqUxY8ZowYIFOnz4sG699dZYHA4AAHQjMQkfP/rRj/T1119r7ty5qqio0IUXXqilS5e2mIQKAADcJybrfJwJO9f5AAAA1nB0nQ8AAIC2ED4AAICtCB8AAMBWhA8AAGArwgcAALAV4QMAANgqJut8nInjV/5ygzkAALqP45/b7VnBo8uFj+rqakniBnMAAHRD1dXVysjIaLNNl1tkLBwOa9++fUpLS5PH47H0vYPBoPLz81VeXu6KBcyoN765rV7JfTVTb3yLt3qNMaqurlZeXp683rZndXS5kQ+v16v+/fvH9Bhuu4Ed9cY3t9Urua9m6o1v8VTv6UY8jmPCKQAAsBXhAwAA2MpV4cPv9+uBBx6Q3+93uiu2oN745rZ6JffVTL3xzW31NtflJpwCAID45qqRDwAA4DzCBwAAsBXhAwAA2IrwAQAAbOWa8PH4449r4MCBSk5O1tixY7V27Vqnu9QpJSUlGj16tNLS0pSdna0bbrhB27Zti2pTW1ur4uJi9e7dWz179tQ//dM/af/+/VFt9uzZo0mTJik1NVXZ2dmaMWOGGhoa7CylU+bPny+Px6Pp06dHtsVbvXv37tUtt9yi3r17KyUlRcOGDdP69esj+40xmjt3rvr27auUlBQVFRVpx44dUe9x8OBBTZ48Wenp6crMzNS//uu/qqamxu5STquxsVFz5sxRYWGhUlJSdM455+ihhx6KujdEd6935cqVuvbaa5WXlyePx6MlS5ZE7beqvi1btuiyyy5TcnKy8vPz9eijj8a6tFa1VW99fb1mzpypYcOGqUePHsrLy9OPf/xj7du3L+o94qXek91xxx3yeDxasGBB1PbuVK9ljAssXrzYJCUlmT//+c/m448/Nj/96U9NZmam2b9/v9Nd67AJEyaYZ555xmzdutVs2rTJfO973zMFBQWmpqYm0uaOO+4w+fn5ZsWKFWb9+vXm4osvNpdccklkf0NDgxk6dKgpKioyGzduNG+99Zbp06ePmT17thMltdvatWvNwIEDzQUXXGDuueeeyPZ4qvfgwYNmwIAB5ic/+YlZs2aN2blzp1m2bJn5/PPPI23mz59vMjIyzJIlS8zmzZvNddddZwoLC83Ro0cjba6++mozfPhws3r1avPBBx+Yb33rW+bmm292oqQ2zZs3z/Tu3du8+eabZteuXebll182PXv2NL///e8jbbp7vW+99Zb5xS9+YV555RUjybz66qtR+62oLxAImJycHDN58mSzdetW88ILL5iUlBTz5JNP2lVmRFv1VlVVmaKiIvPiiy+azz77zJSWlpoxY8aYkSNHRr1HvNTb3CuvvGKGDx9u8vLyzO9+97uofd2pXqu4InyMGTPGFBcXR143NjaavLw8U1JS4mCvrFFZWWkkmffff98Y0/SPOzEx0bz88suRNp9++qmRZEpLS40xTf9YvF6vqaioiLRZuHChSU9PN6FQyN4C2qm6utoMGjTILF++3HznO9+JhI94q3fmzJnm0ksvPeX+cDhscnNzza9//evItqqqKuP3+80LL7xgjDHmk08+MZLMunXrIm3efvtt4/F4zN69e2PX+U6YNGmSue2226K2/eAHPzCTJ082xsRfvSd/OFlV35/+9CeTlZUV9fM8c+ZMc+6558a4ora19WF83Nq1a40ks3v3bmNMfNb75Zdfmn79+pmtW7eaAQMGRIWP7lzvmYj70y51dXUqKytTUVFRZJvX61VRUZFKS0sd7Jk1AoGAJKlXr16SpLKyMtXX10fVO3jwYBUUFETqLS0t1bBhw5STkxNpM2HCBAWDQX388cc29r79iouLNWnSpKi6pPir9/XXX9eoUaN04403Kjs7WyNGjNDTTz8d2b9r1y5VVFRE1ZuRkaGxY8dG1ZuZmalRo0ZF2hQVFcnr9WrNmjX2FdMOl1xyiVasWKHt27dLkjZv3qxVq1Zp4sSJkuKv3pNZVV9paakuv/xyJSUlRdpMmDBB27Zt06FDh2yqpnMCgYA8Ho8yMzMlxV+94XBYU6ZM0YwZMzRkyJAW++Ot3vaK+/Bx4MABNTY2Rn3wSFJOTo4qKioc6pU1wuGwpk+frvHjx2vo0KGSpIqKCiUlJUX+IR/XvN6KiopWvx/H93U1ixcv1oYNG1RSUtJiX7zVu3PnTi1cuFCDBg3SsmXLdOedd2ratGlatGiRpBP9bevnuaKiQtnZ2VH7ExIS1KtXry5X76xZs3TTTTdp8ODBSkxM1IgRIzR9+nRNnjxZUvzVezKr6utOP+PN1dbWaubMmbr55psjN1aLt3ofeeQRJSQkaNq0aa3uj7d626vL3dUW7VdcXKytW7dq1apVTnclZsrLy3XPPfdo+fLlSk5Odro7MRcOhzVq1Cg9/PDDkqQRI0Zo69ateuKJJzR16lSHe2e9l156Sc8995yef/55DRkyRJs2bdL06dOVl5cXl/XihPr6ev3whz+UMUYLFy50ujsxUVZWpt///vfasGGDPB6P093pUuJ+5KNPnz7y+Xwtrn7Yv3+/cnNzHerVmbv77rv15ptv6t1331X//v0j23Nzc1VXV6eqqqqo9s3rzc3NbfX7cXxfV1JWVqbKykpddNFFSkhIUEJCgt5//3394Q9/UEJCgnJycuKq3r59++r888+P2nbeeedpz549kk70t62f59zcXFVWVkbtb2ho0MGDB7tcvTNmzIiMfgwbNkxTpkzRz3/+88goV7zVezKr6utOP+PSieCxe/duLV++POp28vFU7wcffKDKykoVFBREfn/t3r1b9913nwYOHCgpvurtiLgPH0lJSRo5cqRWrFgR2RYOh7VixQqNGzfOwZ51jjFGd999t1599VW98847KiwsjNo/cuRIJSYmRtW7bds27dmzJ1LvuHHj9NFHH0X9wB//BXDyB5/TrrzySn300UfatGlT5DFq1ChNnjw58jye6h0/fnyLS6e3b9+uAQMGSJIKCwuVm5sbVW8wGNSaNWui6q2qqlJZWVmkzTvvvKNwOKyxY8faUEX7HTlyRF5v9K8hn8+ncDgsKf7qPZlV9Y0bN04rV65UfX19pM3y5ct17rnnKisry6Zq2ud48NixY4f+9re/qXfv3lH746neKVOmaMuWLVG/v/Ly8jRjxgwtW7ZMUnzV2yFOz3i1w+LFi43f7zfPPvus+eSTT8ztt99uMjMzo65+6C7uvPNOk5GRYd577z3z1VdfRR5HjhyJtLnjjjtMQUGBeeedd8z69evNuHHjzLhx4yL7j196etVVV5lNmzaZpUuXmrPOOqtLXnramuZXuxgTX/WuXbvWJCQkmHnz5pkdO3aY5557zqSmppq//vWvkTbz5883mZmZ5rXXXjNbtmwx119/fauXZo4YMcKsWbPGrFq1ygwaNKjLXHra3NSpU02/fv0il9q+8sorpk+fPub++++PtOnu9VZXV5uNGzeajRs3Gknmt7/9rdm4cWPk6g4r6quqqjI5OTlmypQpZuvWrWbx4sUmNTXVkUsx26q3rq7OXHfddaZ///5m06ZNUb/Dml/JES/1tubkq12M6V71WsUV4cMYY/74xz+agoICk5SUZMaMGWNWr17tdJc6RVKrj2eeeSbS5ujRo+auu+4yWVlZJjU11Xz/+983X331VdT7fPHFF2bixIkmJSXF9OnTx9x3332mvr7e5mo65+TwEW/1vvHGG2bo0KHG7/ebwYMHm6eeeipqfzgcNnPmzDE5OTnG7/ebK6+80mzbti2qzTfffGNuvvlm07NnT5Oenm5uvfVWU11dbWcZ7RIMBs0999xjCgoKTHJysjn77LPNL37xi6gPou5e77vvvtvqv9mpU6caY6yrb/PmzebSSy81fr/f9OvXz8yfP9+uEqO0Ve+uXbtO+Tvs3XffjbxHvNTbmtbCR3eq1yoeY5otJQgAABBjcT/nAwAAdC2EDwAAYCvCBwAAsBXhAwAA2IrwAQAAbEX4AAAAtiJ8AAAAWxE+AACArQgfAADAVoQPAABgK8IHAACwFeEDAADY6v8DCHRRX0Q4tqwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 :\n",
        "## Implement linear regression using \"mini-batch\" gradient descent\n",
        "\n",
        "\n",
        "Mini-Batch Gradient Descent: Parameters are updated after computing the gradient of  the error with respect to a subset of the training set.\n",
        "Thus, mini-batch gradient descent makes a compromise between the speedy convergence and the noise associated with gradient update which makes it a more flexible and robust algorithm.\n",
        "\n",
        "\n",
        " Mini-Batch Gradient Descent: Algorithm-\n",
        "\n",
        "    Let theta = model parameters and max_iters = number of epochs. for itr = 1, 2, 3, …, max_iters:       for mini_batch (X_mini, y_mini):\n",
        "\n",
        "        Forward Pass on the batch X_mini:\n",
        "            Make predictions on the mini-batch\n",
        "            Compute error in predictions (J(theta)) with the current values of the parameters\n",
        "        Backward Pass:\n",
        "            Compute gradient(theta) = partial derivative of J(theta) w.r.t. theta\n",
        "        Update parameters:\n",
        "            theta = theta – learning_rate*gradient(theta)"
      ],
      "metadata": {
        "id": "I1CbSYwCn0BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# creating data\n",
        "mean = np.array([5.0, 6.0])\n",
        "cov = np.array([[1.0, 0.95], [0.95, 1.2]])\n",
        "data = np.random.multivariate_normal(mean, cov, 8000)\n",
        "\n",
        "# visualising data\n",
        "plt.scatter(data[:500, 0], data[:500, 1], marker='.')\n",
        "plt.show()\n",
        "\n",
        "# train-test-split\n",
        "data = np.hstack((np.ones((data.shape[0], 1)), data))\n",
        "\n",
        "split_factor = 0.90\n",
        "split = int(split_factor * data.shape[0])\n",
        "\n",
        "X_train = data[:split, :-1]\n",
        "y_train = data[:split, -1].reshape((-1, 1))\n",
        "X_test = data[split:, :-1]\n",
        "y_test = data[split:, -1].reshape((-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "# linear regression using \"mini-batch\" gradient descent\n",
        "# function to compute hypothesis / predictions\n",
        "\n",
        "\n",
        "def hypothesis(X, theta):\n",
        "\treturn '...'\n",
        "\n",
        "# function to compute gradient of error function w.r.t. theta\n",
        "\n",
        "\n",
        "def gradient(X, y, theta):\n",
        "\t'...'\n",
        "\n",
        "# function to compute the error for current values of theta\n",
        "\n",
        "\n",
        "def cost(X, y, theta):\n",
        "\t'...'\n",
        "\n",
        "# function to create a list containing mini-batches\n",
        "\n",
        "\n",
        "def create_mini_batches(X, y, batch_size):\n",
        "\tmini_batches = []\n",
        "\tdata = np.hstack((X, y))\n",
        "\tnp.random.shuffle(data)\n",
        "\tn_minibatches = data.shape[0] // batch_size\n",
        "\ti = 0\n",
        "\n",
        "\tfor i in range(n_minibatches + 1):\n",
        "\t\t'...'\n",
        "\tif data.shape[0] % batch_size != 0:\n",
        "\t\t'...'\n",
        "\treturn '...'\n",
        "\n",
        "# function to perform mini-batch gradient descent\n",
        "\n",
        "\n",
        "def gradientDescent(X, y, learning_rate=0.001, batch_size=32):\n",
        "\t'...'\n",
        "\n",
        "\n",
        "theta, error_list = gradientDescent(X_train, y_train)\n",
        "print(\"Bias = \", theta[0])\n",
        "print(\"Coefficients = \", theta[1:])\n",
        "\n",
        "# visualising gradient descent\n",
        "plt.plot(error_list)\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# predicting output for X_test\n",
        "y_pred = hypothesis(X_test, theta)\n",
        "plt.scatter(X_test[:, 1], y_test[:, ], marker='.')\n",
        "plt.plot(X_test[:, 1], y_pred, color='orange')\n",
        "plt.show()\n",
        "\n",
        "# calculating error in predictions\n",
        "error = '...'\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz46YIVYseN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}